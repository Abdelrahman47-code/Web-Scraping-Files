{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00868b19",
   "metadata": {},
   "source": [
    "# Codes of Scraping Images from Different Websites\n",
    "\n",
    "Make sure to write the url correctly and the destinations folders to save the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c9e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd502369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download an image\n",
    "def download_image(image_url, destination_folder, image_name):\n",
    "    try:\n",
    "        response = requests.get(image_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            # Create the destination path\n",
    "            destination_path = os.path.join(destination_folder, image_name)\n",
    "\n",
    "            # Save the image\n",
    "            with open(destination_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=128):\n",
    "                    file.write(chunk)\n",
    "\n",
    "            print(f\"Image downloaded: {image_name}\")\n",
    "        else:\n",
    "            print(f\"Failed to download image from {image_url}. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d3884",
   "metadata": {},
   "source": [
    "# Getty Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2c282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Number of pages to scrape\n",
    "num_pages = 1000\n",
    "counter = 0\n",
    "website_name = 'getty'\n",
    "\n",
    "try:\n",
    "    # Create a folder to save downloaded images\n",
    "    download_folder = 'Mosques_Images/Alaqsa_Mosque'\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    # Create a CSV file to store information\n",
    "    csv_file_path = f'Mosques_Images/Alaqsa_Mosque_Images_Data_{website_name}.csv'\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['ID', 'Image Name', 'Image URL']\n",
    "        csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        csv_writer.writeheader()\n",
    "\n",
    "        # Iterate over pages\n",
    "        for page_num in range(1, num_pages + 1):\n",
    "            # Navigate to the Getty Images page\n",
    "            driver.get(f\"https://www.gettyimages.in/photos/al-aqsa-mosque?assettype=image&sort=mostpopular&phrase=al%20aqsa%20mosque&license=rf%2Crm&page={page_num}\")\n",
    "\n",
    "            # Wait for some time to allow dynamic content to load\n",
    "            time.sleep(5)\n",
    "\n",
    "            # Find all image elements on the page\n",
    "            image_elements = driver.find_elements(By.CLASS_NAME, 'BLA_wBUJrga_SkfJ8won')\n",
    "            if len(image_elements) == 0:\n",
    "                print(f'Page {page_num} is the Final Page')\n",
    "                break\n",
    "\n",
    "            # Download and save images, and write information to the CSV file\n",
    "            for i, element in enumerate(image_elements, start=1):\n",
    "                image_url = element.get_attribute(\"src\")\n",
    "                image_name = f\"image_{i}_page_{page_num}_{website_name}.jpg\"\n",
    "                download_image(image_url, download_folder, image_name)\n",
    "\n",
    "                # Write information to CSV file\n",
    "                csv_writer.writerow({'ID': i, 'Image Name': image_name, 'Image URL': image_url})\n",
    "                counter += 1\n",
    "\n",
    "            print(f'\\n***** Page {page_num} Scrapped Successfully *****\\n')\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "    print(f'Number of the Scraped Images is {counter}')\n",
    "    print(f\"CSV file '{csv_file_path}' created successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9aa64",
   "metadata": {},
   "source": [
    "# iStock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4799de5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Number of pages to scrape\n",
    "num_pages = 1000\n",
    "counter = 0\n",
    "website_name = 'istock'\n",
    "\n",
    "try:    \n",
    "    # Create a folder to save downloaded images\n",
    "    download_folder = 'Mosques_Images/Prophet_Mosque4'\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    # Create a CSV file to store information\n",
    "    csv_file_path = f'Mosques_Images/Prophet_Mosque_Images_Data_{website_name}.csv'\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['ID', 'Image Name', 'Image URL']\n",
    "        csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        csv_writer.writeheader()\n",
    "\n",
    "        # Iterate over pages\n",
    "        for page_num in range(1, num_pages + 1):\n",
    "            # Navigate to the Shutterstock page\n",
    "            driver.get(f\"https://www.istockphoto.com/search/2/image-film?phrase=prophet%20mosque&page={page_num}\")\n",
    "            \n",
    "            # Wait for the page to load\n",
    "            driver.implicitly_wait(5)\n",
    "\n",
    "            # Find all image elements on the page\n",
    "            image_elements = driver.find_elements(By.CLASS_NAME, \"yGh0CfFS4AMLWjEE9W7v\")\n",
    "\n",
    "            if len(image_elements) == 0:\n",
    "                print(f'Page {page_num} is the Final Page')\n",
    "                break\n",
    "\n",
    "            # Download and save images, and write information to the CSV file\n",
    "            for i, element in enumerate(image_elements, start=1):\n",
    "                image_url = element.get_attribute(\"src\")\n",
    "                image_name = f\"image_{i}_page_{page_num}_{website_name}.jpg\"\n",
    "                download_image(image_url, download_folder, image_name)\n",
    "\n",
    "                # Write information to CSV file\n",
    "                csv_writer.writerow({'ID': i, 'Image Name': image_name, 'Image URL': image_url})\n",
    "                counter += 1\n",
    "            \n",
    "            print(f'\\n***** Page {page_num} Scrapped Successfully *****\\n')\n",
    "\n",
    "    print(f\"Number of Scraped Images: {counter}\")\n",
    "    print(f\"CSV file '{csv_filename}' created successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f8aa7",
   "metadata": {},
   "source": [
    "# Shutterstock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Number of pages to scrape\n",
    "num_pages = 1000\n",
    "counter = 0\n",
    "website_name = 'shutterstock'\n",
    "\n",
    "try:\n",
    "    # Create a folder to save downloaded images\n",
    "    download_folder = 'Mosques_Images/Scared_Mosque'\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    # Create a CSV file to store information\n",
    "    csv_file_path = f'Mosques_Images/Scared_Mosque_Images_Data_{website_name}.csv'\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['ID', 'Image Name', 'Image URL']\n",
    "        csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        csv_writer.writeheader()\n",
    "\n",
    "        # Iterate over pages\n",
    "        for page_num in range(1, num_pages + 1):\n",
    "            # Navigate to the Shutterstock page\n",
    "            driver.get(f\"https://www.shutterstock.com/search/mecca?page={page_num}\")\n",
    "\n",
    "            # Wait for some time to allow dynamic content to load\n",
    "            time.sleep(5)\n",
    "\n",
    "            # Find all image elements on the page\n",
    "            image_elements = driver.find_elements(By.CLASS_NAME, \"mui-1l7n00y-thumbnail\")\n",
    "\n",
    "            if len(image_elements) == 0:\n",
    "                print(f'Page {page_num} is the Final Page')\n",
    "                break\n",
    "\n",
    "            # Download and save images, and write information to the CSV file\n",
    "            for i, element in enumerate(image_elements, start=1):\n",
    "                image_url = element.get_attribute(\"src\")\n",
    "                image_name = f\"image_{i}_page_{page_num}_{website_name}.jpg\"\n",
    "                download_image(image_url, download_folder, image_name)\n",
    "\n",
    "                # Write information to CSV file\n",
    "                csv_writer.writerow({'ID': i, 'Image Name': image_name, 'Image URL': image_url})\n",
    "                counter += 1\n",
    "                print(f'\\n***** Page {page_num} Scrapped Successfully *****\\n')\n",
    "\n",
    "    print(f'Number of the Scraped Images is {counter}')\n",
    "    print(f\"CSV file '{csv_file_path}' created successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25544c06",
   "metadata": {},
   "source": [
    "# Dreamstime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7858636e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Number of pages to scrape\n",
    "num_pages = 1000\n",
    "counter = 0\n",
    "website_name = 'dreamstime'\n",
    "\n",
    "try:\n",
    "    # Create a folder to save downloaded images\n",
    "    download_folder = 'Mosques_Images/Alazhar_Mosque'\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    # Create a CSV file to store information\n",
    "    csv_file_path = f'Mosques_Images/Alazhar_Mosque_Images_Data_{website_name}.csv'\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['ID', 'Image Name', 'Image URL']\n",
    "        csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        csv_writer.writeheader()\n",
    "\n",
    "        # Iterate over pages\n",
    "        for page_num in range(1, num_pages + 1):\n",
    "            # Navigate to the Dreamstime page\n",
    "            driver.get(f\"https://www.dreamstime.com/photos-images/al-azhar-mosque.html?pg={page_num}\")\n",
    "\n",
    "            # Wait for some time to allow dynamic content to load\n",
    "            time.sleep(5)\n",
    "\n",
    "            # Get the page source and parse it with BeautifulSoup\n",
    "            html_content = driver.page_source\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "            # Find all image elements on the page\n",
    "            image_elements = soup.find_all('img', class_='item__thumb')\n",
    "\n",
    "            if len(image_elements) == 0:\n",
    "                print(f'Page {page_num} is the Final Page')\n",
    "                break\n",
    "\n",
    "            # Download and save images, and write information to the CSV file\n",
    "            for i, img in enumerate(image_elements, start=1):\n",
    "                image_url = img['src']\n",
    "                image_name = f\"image_{i}_page_{page_num}_{website_name}.jpg\"\n",
    "                download_image(image_url, download_folder, image_name)\n",
    "\n",
    "                # Write information to CSV file\n",
    "                csv_writer.writerow({'ID': i, 'Image Name': image_name, 'Image URL': image_url})\n",
    "                counter += 1\n",
    "            print(f'\\n***** Page {page_num} Scrapped Successfully *****\\n')\n",
    "\n",
    "    print(f'Number of the Scraped Images is {counter}')\n",
    "    print(f\"CSV file '{csv_file_path}' created successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8006a0db",
   "metadata": {},
   "source": [
    "# Renaming the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00adb3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the folder containing the images\n",
    "folder_path = 'Mosques_Images/Alazhar_Mosque'\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Filter out non-image files\n",
    "allowed_extensions = ['.jpg', '.jpeg', '.png']\n",
    "image_files = [file for file in files if any(file.lower().endswith(ext) for ext in allowed_extensions)]\n",
    "\n",
    "\n",
    "# Number of the images\n",
    "num_images = len(image_files)\n",
    "print(f'\\n***** Number of the Images in the Folder is: {num_images} *****\\n')\n",
    "\n",
    "# Sort the image files alphabetically\n",
    "image_files.sort()\n",
    "\n",
    "# Find the number of digits needed for padding\n",
    "num_digits = len(str(num_images))\n",
    "\n",
    "# Rename the image files with sequential numbers\n",
    "for i, old_name in enumerate(image_files, start=1):\n",
    "    # Extract the file extension\n",
    "    _, extension = os.path.splitext(old_name)\n",
    "\n",
    "    # Check if the new name already exists\n",
    "    if os.path.exists(os.path.join(folder_path, new_name)):\n",
    "        # If the new name exists, append a unique identifier\n",
    "        new_name = f\"{i:0{num_digits}d}_conflict{extension}\"\n",
    "\n",
    "    # Create the full paths for the old and new names\n",
    "    old_path = os.path.join(folder_path, old_name)\n",
    "    new_path = os.path.join(folder_path, new_name)\n",
    "\n",
    "    # Rename the file\n",
    "    os.rename(old_path, new_path)\n",
    "\n",
    "# Rename the image files with sequential numbers again to make them from 1 to the number of the last image\n",
    "for i, old_name in enumerate(image_files, start=1):\n",
    "    # Extract the file extension\n",
    "    _, extension = os.path.splitext(old_name)\n",
    "\n",
    "    new_name = f\"{i}{extension}\"\n",
    "    \n",
    "    # Create the full paths for the old and new names\n",
    "    old_path = os.path.join(folder_path, old_name)\n",
    "    new_path = os.path.join(folder_path, new_name)\n",
    "\n",
    "    # Rename the file\n",
    "    os.rename(old_path, new_path)\n",
    "\n",
    "    print(f\"Renamed: {old_name} to {new_name}\")\n",
    "    \n",
    "print(f\"\\n***** All Images Renamed Successfully in '{folder_path}' Folder *****\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7518d289",
   "metadata": {},
   "source": [
    "# Made by: Abdelrahman Eldaba 👨‍💻\n",
    "\n",
    "Check out my portfolio [Here](https://www.linkedin.com/posts/abdelrahman-eldaba-739805192_datascience-dataanalysis-webscraping-activity-7156428468711219201-gtWA/) 🌟\n",
    "\n",
    "Connect with me on [LinkedIn](https://www.linkedin.com/in/abdelrahman-eldaba-739805192/) 🌐\n",
    "\n",
    "Look at my [GitHub](https://github.com/Abdelrahman47-code) 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
